{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langdetect import detect\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from dictionary import contraction_map, unnecessary_patterns\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "sample = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "train_y = train.as_matrix()[:, 2:].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_object = re.compile(\"|\".join(contraction_map.keys()))\n",
    "sub_patterns = '|'.join(unnecessary_patterns)\n",
    "\n",
    "def expand_contraction(sentence, sub_map=contraction_map, sub_object=contraction_object):\n",
    "    def matching_case(match):\n",
    "        return sub_map[match.group(0)]    \n",
    "    return sub_object.sub(matching_case, sentence)\n",
    "\n",
    "\n",
    "def cleaning_text(text, sub_patterns=sub_patterns):\n",
    "    text = re.sub(sub_patterns, ' ', text)\n",
    "    text = re.sub('[0-9]+', 'NUM', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def tokenizer(sentence):\n",
    "    tokenized_sentence = word_tokenize(sentence)\n",
    "    return [wnl.lemmatize(token) for token in tokenized_sentence if token not in stop_words]\n",
    "\n",
    "\n",
    "def preprocessing(dataset):\n",
    "    comment_list = []\n",
    "    for comment in dataset.comment_text:\n",
    "        comment_list.append(cleaning_text(expand_contraction(comment)))\n",
    "    return comment_list\n",
    "\n",
    "\n",
    "def persistence(fname, mode='load', obj=None):\n",
    "    if mode == 'load':\n",
    "        with open(fname, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    elif mode == 'save' and obj is not None:\n",
    "        with open(fname, 'wb') as f:\n",
    "            pickle.dump(obj, f)\n",
    "            \n",
    "\n",
    "def make_submission(sample_sub, fname, prediction):\n",
    "    idx = sample['id']\n",
    "    columns = sample.columns.tolist()[1:]\n",
    "    sub = pd.DataFrame(prediction, index=idx, columns=columns)\n",
    "    sub.to_csv('submissions/{}.csv'.format(fname), index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_comment = persistence('train_comment.pkl', 'load')\n",
    "test_comment = persistence('test_comment.pkl', 'load')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 7145)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=tokenizer, min_df=50, max_df=0.5, stop_words=stop_words, lowercase=True)\n",
    "train_x = tfidf.fit_transform(train_comment).todense()\n",
    "\n",
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = persistence('tfidf_model.pkl', 'load')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6166"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.vocabulary_['suck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = tfidf.transform(train_comment).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153164, 7145)\n"
     ]
    }
   ],
   "source": [
    "test_x = tfidf.transform(test_comment).todense()\n",
    "\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "persistence('tfidf_model.pkl', 'save', tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153164, 6)\n"
     ]
    }
   ],
   "source": [
    "model = OneVsRestClassifier(LogisticRegression(class_weight='balanced'))\n",
    "# score = cross_val_score(model, train_x, train_y, scoring=\"roc_auc\", cv=cv)\n",
    "# print(round(np.mean(score), 4))\n",
    "\n",
    "model.fit(train_x, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153164, 6)\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict_proba(test_x)\n",
    "\n",
    "print(prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submission(sample, 'tfidf_lr_balanced', prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OneVsRestClassifier(DecisionTreeClassifier())\n",
    "score = cross_val_score(model, train_x, train_y, scoring=\"roc_auc\", cv=cv)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "glove_file = datapath('/Users/youncheol/Documents/projects/toxic-comment-classification-challenge/embedding/glove.twitter.27B.100d.txt')\n",
    "tmp_file = get_tmpfile('glove_model.txt')\n",
    "\n",
    "glove2word2vec(glove_file, tmp_file)\n",
    "\n",
    "glove_model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159000"
     ]
    }
   ],
   "source": [
    "train_tokens = []\n",
    "cnt = 0\n",
    "\n",
    "for comment in train_comment:\n",
    "    comment = comment.lower()\n",
    "    train_tokens.append(tokenizer(comment))\n",
    "    cnt += 1\n",
    "    if (cnt != 0) and (cnt % 1000 == 0):\n",
    "        sys.stdout.write('\\r{}'.format(cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153000"
     ]
    }
   ],
   "source": [
    "test_tokens = []\n",
    "cnt = 0\n",
    "\n",
    "for comment in test_comment:\n",
    "    comment = comment.lower()\n",
    "    test_tokens.append(tokenizer(comment))\n",
    "    cnt += 1\n",
    "    if (cnt != 0) and (cnt % 1000 == 0):\n",
    "        sys.stdout.write('\\r{}'.format(cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_all = TfidfVectorizer(tokenizer=tokenizer, stop_words=stop_words, lowercase=True)\n",
    "tfidf_matrix = tfidf_all.fit_transform(train_comment).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'as'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-18a81937e85a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfidf_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'as'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'as'"
     ]
    }
   ],
   "source": [
    "tfidf_matrix[0, tfidf_all.vocabulary_['as']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sentence_vector(sentence, dim):\n",
    "    sentence_vector = np.zeros(dim)\n",
    "    num_of_tokens = 0\n",
    "    for token in sentence:\n",
    "        try:\n",
    "            sentence_vector = np.add(sentence_vector, glove_model.get_vector(token))\n",
    "            num_of_tokens += 1\n",
    "        except:\n",
    "            continue\n",
    "    if num_of_tokens > 0:\n",
    "        sentence_vector = np.divide(sentence_vector, num_of_tokens)\n",
    "    return sentence_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_weighted_sentence_vector(sentence, dim, i):\n",
    "    sentence_vector = np.zeros(dim)\n",
    "    sum_of_weights = 0\n",
    "    for token in sentence:\n",
    "        try:\n",
    "            tfidf_weight = tfidf_matrix[i, tfidf_all.vocabulary_[token]]\n",
    "            sentence_vector = np.add(sentence_vector, tfidf_weight * glove_model.get_vector(token))\n",
    "            sum_of_weights += tfidf_weight\n",
    "        except:\n",
    "            continue\n",
    "    if sum_of_weights > 0:\n",
    "        sentence_vector = np.divide(sentence_vector, sum_of_weights)\n",
    "    return sentence_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 100\n",
    "\n",
    "train_x = np.zeros([len(train_tokens), dim])\n",
    "\n",
    "for i, sentence in enumerate(train_tokens):\n",
    "    train_x[i] = make_weighted_sentence_vector(sentence, dim, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 100\n",
    "\n",
    "test_x = np.zeros([len(test_tokens), dim])\n",
    "\n",
    "for i, sentence in enumerate(test_tokens):\n",
    "    test_x[i] = make_weighted_sentence_vector(sentence, dim, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 100)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95532909 0.95491295 0.95404358 0.95282175 0.95314141]\n"
     ]
    }
   ],
   "source": [
    "model = OneVsRestClassifier(LogisticRegression(class_weight='balanced'))\n",
    "score = cross_val_score(model, train_x, train_y, scoring=\"roc_auc\", cv=cv)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.83593426 0.84916013 0.85917871 0.85198862 0.85241564]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = OneVsRestClassifier(DecisionTreeClassifier(max_depth=8, class_weight='balanced'), n_jobs=3)\n",
    "score = cross_val_score(model, train_x, train_y, scoring=\"roc_auc\", cv=cv)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9485343842498016\n",
      "0.9505735267664162\n",
      "0.9514172787341217\n"
     ]
    }
   ],
   "source": [
    "depth_list = [30, 40, 50]\n",
    "\n",
    "for depth in depth_list:\n",
    "    model = OneVsRestClassifier(RandomForestClassifier(n_estimators=depth, max_depth=9, class_weight='balanced'), n_jobs=3)\n",
    "    model.fit(x_train, y_train)\n",
    "    pred = model.predict_proba(x_test)\n",
    "    print(roc_auc_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OneVsRestClassifier(RandomForestClassifier(n_estimators=50, max_depth=9, class_weight='balanced'), n_jobs=3)\n",
    "model.fit(train_x, train_y)\n",
    "prediction = model.predict_proba(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submission(sample, '100vector_rf', prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-18a12d9245bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneVsRestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGradientBoostingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/multiclass.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;34m\"not %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_binarizer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                 self.label_binarizer_.classes_[i]])\n\u001b[0;32m--> 215\u001b[0;31m             for i, column in enumerate(columns))\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = OneVsRestClassifier(GradientBoostingClassifier(), n_jobs=3)\n",
    "model.fit(x_train, y_train)\n",
    "pred = model.predict_proba(x_test)\n",
    "print(roc_auc_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92976895 0.92987358 0.93314031 0.93257189 0.92671809]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = OneVsRestClassifier(RandomForestClassifier(max_depth=10, class_weight='balanced'), n_jobs=3)\n",
    "score = cross_val_score(model, train_x, train_y, scoring=\"roc_auc\", cv=cv)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-dca1a0ef4682>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneVsRestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'poly'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/multiclass.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;34m\"not %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_binarizer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                 self.label_binarizer_.classes_[i]])\n\u001b[0;32m--> 215\u001b[0;31m             for i, column in enumerate(columns))\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = OneVsRestClassifier(SVC(kernel='poly', probability=True, class_weight='balanced', verbose=True), n_jobs=3)\n",
    "model.fit(x_train, y_train)\n",
    "pred = model.predict_proba(x_test)\n",
    "\n",
    "roc_auc_score(y_test, pred)\n",
    "# score = cross_val_score(model, train_x, train_y, scoring=\"roc_auc\", cv=cv)\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=train_x.shape[1]-1)\n",
    "svd.fit(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = svd.transform(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1514199642787458"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(svd.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(svd.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OneVsRestClassifier(LogisticRegression())\n",
    "score = cross_val_score(model, X, train_y, scoring=\"roc_auc\", cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9486816  0.94820201 0.9493849  0.94904128 0.94804557]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model = OneVsRestClassifier(LinearSVC())\n",
    "score = cross_val_score(model, X, train_y, scoring=\"roc_auc\", cv=cv)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB + BOW\n",
      "min_df: 50, vacabulary: 7145, score: 0.9428 Mon Jun 25 00:32:06 2018\n",
      "min_df: 60, vacabulary: 6369, score: 0.9431 Mon Jun 25 00:33:54 2018\n",
      "min_df: 70, vacabulary: 5804, score: 0.9434 Mon Jun 25 00:35:44 2018\n",
      "min_df: 80, vacabulary: 5340, score: 0.943 Mon Jun 25 03:00:30 2018\n",
      "min_df: 90, vacabulary: 4942, score: 0.9433 Mon Jun 25 04:49:35 2018\n",
      "min_df: 100, vacabulary: 4605, score: 0.9431 Mon Jun 25 06:35:14 2018\n",
      "\n",
      "LogisticRegression + BOW\n",
      "min_df: 50, vacabulary: 7145, score: 0.9425 Mon Jun 25 06:39:46 2018\n",
      "min_df: 60, vacabulary: 6369, score: 0.9423 Mon Jun 25 06:44:08 2018\n",
      "min_df: 70, vacabulary: 5804, score: 0.9426 Mon Jun 25 06:48:31 2018\n",
      "min_df: 80, vacabulary: 5340, score: 0.9421 Mon Jun 25 06:52:54 2018\n",
      "min_df: 90, vacabulary: 4942, score: 0.9412 Mon Jun 25 06:57:15 2018\n",
      "min_df: 100, vacabulary: 4605, score: 0.9408 Mon Jun 25 07:01:32 2018\n",
      "\n",
      "MultinomialNB + TF-IDF\n",
      "min_df: 50, vacabulary: 7145, score: 0.9551 Mon Jun 25 07:03:20 2018\n",
      "min_df: 60, vacabulary: 6369, score: 0.9569 Mon Jun 25 07:05:08 2018\n",
      "min_df: 70, vacabulary: 5804, score: 0.9582 Mon Jun 25 07:06:56 2018\n",
      "min_df: 80, vacabulary: 5340, score: 0.9586 Mon Jun 25 07:08:45 2018\n",
      "min_df: 90, vacabulary: 4942, score: 0.9595 Mon Jun 25 07:10:33 2018\n",
      "min_df: 100, vacabulary: 4605, score: 0.96 Mon Jun 25 07:12:21 2018\n",
      "\n",
      "LogisticRegression + TF-IDF\n",
      "min_df: 50, vacabulary: 7145, score: 0.9775 Mon Jun 25 07:14:33 2018\n",
      "min_df: 60, vacabulary: 6369, score: 0.9774 Mon Jun 25 07:16:44 2018\n",
      "min_df: 70, vacabulary: 5804, score: 0.9771 Mon Jun 25 07:18:55 2018\n",
      "min_df: 80, vacabulary: 5340, score: 0.9763 Mon Jun 25 07:21:07 2018\n",
      "min_df: 90, vacabulary: 4942, score: 0.976 Mon Jun 25 07:23:18 2018\n",
      "min_df: 100, vacabulary: 4605, score: 0.9753 Mon Jun 25 07:25:29 2018\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "min_df_list = [50, 60, 70, 80, 90, 100]\n",
    "\n",
    "print('MultinomialNB + BOW')\n",
    "for min_df in min_df_list:\n",
    "    bow = CountVectorizer(tokenizer=tokenizer, min_df=min_df, max_df=0.5, stop_words=stop_words, lowercase=True)\n",
    "    X = bow.fit_transform(train_comment)\n",
    "    model = OneVsRestClassifier(MultinomialNB())\n",
    "    score = cross_val_score(model, X, train_y, scoring=\"roc_auc\", cv=cv)\n",
    "    print('min_df: {}, vacabulary: {}, score: {} {}'.format(min_df, len(bow.vocabulary_), round(np.mean(score), 4), time.ctime()))\n",
    "\n",
    "print()\n",
    "print('LogisticRegression + BOW')\n",
    "for min_df in min_df_list:\n",
    "    bow = CountVectorizer(tokenizer=tokenizer, min_df=min_df, max_df=0.5, stop_words=stop_words, lowercase=True)\n",
    "    X = bow.fit_transform(train_comment)\n",
    "    model = OneVsRestClassifier(LogisticRegression())\n",
    "    score = cross_val_score(model, X, train_y, scoring=\"roc_auc\", cv=cv)\n",
    "    print('min_df: {}, vacabulary: {}, score: {} {}'.format(min_df, len(bow.vocabulary_), round(np.mean(score), 4), time.ctime()))\n",
    "    \n",
    "print()\n",
    "print('MultinomialNB + TF-IDF')\n",
    "for min_df in min_df_list:\n",
    "    bow = TfidfVectorizer(tokenizer=tokenizer, min_df=min_df, max_df=0.5, stop_words=stop_words, lowercase=True)\n",
    "    X = bow.fit_transform(train_comment)\n",
    "    model = OneVsRestClassifier(MultinomialNB())\n",
    "    score = cross_val_score(model, X, train_y, scoring=\"roc_auc\", cv=cv)\n",
    "    print('min_df: {}, vacabulary: {}, score: {} {}'.format(min_df, len(bow.vocabulary_), round(np.mean(score), 4), time.ctime()))\n",
    "    \n",
    "print()\n",
    "print('LogisticRegression + TF-IDF')\n",
    "for min_df in min_df_list:\n",
    "    bow = TfidfVectorizer(tokenizer=tokenizer, min_df=min_df, max_df=0.5, stop_words=stop_words, lowercase=True)\n",
    "    X = bow.fit_transform(train_comment)\n",
    "    model = OneVsRestClassifier(LogisticRegression())\n",
    "    score = cross_val_score(model, X, train_y, scoring=\"roc_auc\", cv=cv)\n",
    "    print('min_df: {}, vacabulary: {}, score: {} {}'.format(min_df, len(bow.vocabulary_), round(np.mean(score), 4), time.ctime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          n_jobs=1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = TfidfVectorizer(tokenizer=tokenizer, min_df=50, max_df=0.5, ngram_range=(1, 2), stop_words=stop_words, lowercase=True)\n",
    "X = bow.fit_transform(train_comment)\n",
    "model = OneVsRestClassifier(LogisticRegression())\n",
    "# score = cross_val_score(model, X, train_y, scoring=\"roc_auc\", cv=cv)\n",
    "# print(score)\n",
    "# print(round(np.mean(score), 4))\n",
    "\n",
    "model.fit(X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97754792 0.97777691 0.97635416 0.97919322 0.97691912]\n",
      "0.9776\n"
     ]
    }
   ],
   "source": [
    "bow = TfidfVectorizer(tokenizer=tokenizer, min_df=50, max_df=0.5, ngram_range=(1, 3), stop_words=stop_words, lowercase=True)\n",
    "X = bow.fit_transform(train_comment)\n",
    "model = OneVsRestClassifier(LogisticRegression())\n",
    "score = cross_val_score(model, X, train_y, scoring=\"roc_auc\", cv=cv)\n",
    "print(score)\n",
    "print(round(np.mean(score), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15326"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(bow.vocabulary_)\n",
    "# joblib.dump(bow, 'bow_model.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "np.save('bow_train', train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 19575)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = bow.transform(test_comment).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "          n_jobs=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bow_nb.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(clf, 'bow_nb.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba = clf.predict_proba(bow_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_comment.pkl', 'rb') as f:\n",
    "    test_comment = pickle.load(f)\n",
    "    \n",
    "# bow = joblib.load('bow_model.pkl')\n",
    "# clf = joblib.load('bow_nb.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = bow.transform(test_comment).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = clf.predict_proba(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 6)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99984853, 0.92641029, 0.99933135, 0.00160257, 0.99823054,\n",
       "       0.00928929])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_proba[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1, 0], dtype=int32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8939594287182508"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(train_y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = bow.transform(test_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict_proba(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 6)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.columns.tolist()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = sample['id']\n",
    "columns = sample.columns.tolist()[1:]\n",
    "fname = 'tfidf_lr'\n",
    "\n",
    "sub = pd.DataFrame(prediction, index=idx, columns=columns)\n",
    "sub.to_csv('submissions/{}.csv'.format(fname), index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yo bitch Ja Rule is more succesful then you will ever be whats up with you and hating you sad mofuckas   i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me  Ja rule is about pride in da music man  dont diss that shit on him  and nothin is wrong bein like tupac he was a brother too   fuckin white boys get things right next time'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_comment[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = OneVsRestClassifier(SVC())\n",
    "\n",
    "score = cross_val_score(model, X, train_y, scoring=\"roc_auc\", cv=cv)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 13317)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you can do all you are doing right now but if you get a username you will be able to do more and have more impact is what i am saying  and you seem to be very familiar with everything so you probably have a username  just get one  it takes NUM seconds'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaning_text(expand_contraction(train.comment_text[99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['right',\n",
       " 'get',\n",
       " 'username',\n",
       " 'able',\n",
       " 'impact',\n",
       " 'saying',\n",
       " 'seem',\n",
       " 'familiar',\n",
       " 'everything',\n",
       " 'probably',\n",
       " 'username',\n",
       " 'get',\n",
       " 'one',\n",
       " 'take',\n",
       " '10',\n",
       " 'second']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizing_stemming(comment_list[99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "comment_list = []\n",
    "\n",
    "pattern = re.compile('REDIRECT Talk:')\n",
    "\n",
    "for i in range(len(train.comment_text)):\n",
    "    if '(UTC)' in train.comment_text[i]:\n",
    "        comment_list.append(i)\n",
    "        cnt += 1\n",
    "        \n",
    "    if cnt > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['13:0', '18-9']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = 'fadhfkalgjkalfjakl;fjkald 13:0 fafj 18-9 akl;f'\n",
    "time_pattern = re.compile('[0-9]+:[0-9]+')\n",
    "\n",
    "re.findall(time_pattern, string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_pattern = re.compile('[0-9]+\\s\\w+\\s[0-9]+\\s\\(UTC\\)')\n",
    "time_pattern = re.compile('[0-9]+:[0-9]+')\n",
    "ip_pattern = re.compile('[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+')\n",
    "fname_pattern = re.compile('.+\\.jpg|.+\\.jpeg|.+\\.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explanation why the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since i'm retired now.\n",
      "********************\n",
      "d'aww! he matches this background colour i'm seemingly stuck with. thanks.   , january 11, 2016 (utc)\n",
      "********************\n",
      "hey man, i'm really not trying to edit war. it's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. he seems to care more about the formatting than the actual info.\n",
      "********************\n",
      "\" more i can't make any real suggestions on improvement - i wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -i think the references may need tidying so that they are all in the exact same format ie date format etc. i can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.  there appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up. it's listed in the relevant form eg wikipedia:good_article_nominations#transport  \"\n",
      "********************\n",
      "you, sir, are my hero. any chance you remember what page that's on?\n",
      "********************\n",
      "\"  congratulations from me as well, use the tools well.  talk \"\n",
      "********************\n",
      "cocksucker before you piss around on my work\n",
      "********************\n",
      "your vandalism to the matt shirvington article has been reverted.  please don't do it again, or you will be banned.\n",
      "********************\n",
      "sorry if the word 'nonsense' was offensive to you. anyway, i'm not intending to write anything in the article(wow they would jump on me for vandalism), i'm merely requesting that it be more encyclopedic so one can use it for school as a reference. i have been to the selective breeding page but it's almost a stub. it points to 'animal breeding' which is a short messy article that gives you no info. there must be someone around with expertise in eugenics?\n",
      "********************\n",
      "alignment on this subject and which are contrary to those of dulithgow\n",
      "********************\n",
      ". i noticed that the file's description page currently doesn't specify who created the content, so the copyright status is unclear. if you did not create this file yourself, then you will need to specify the owner of the copyright. if you obtained it from a website, then a link to the website from which it was taken, together with a restatement of that website's terms of use of its content, is usually sufficient information. however, if the copyright holder is different from the website's publisher, then their copyright should also be acknowledged.  as well as adding the source, please add a proper copyright licensing tag if the file doesn't have one already. if you created/took the picture, audio, or video then the  tag can be used to release it under the gfdl. if you believe the media meets the criteria at wikipedia:fair use, use a tag such as  or one of the other tags listed at wikipedia:image copyright tags#fair use. see wikipedia:image copyright tags for the full list of copyright tags that you can use.  if you have uploaded other files, consider checking that you have specified their source and tagged them, too. you can find a list of files you have uploaded by following [ this link]. unsourced and untagged images may be deleted one week after they have been tagged, as described on criteria for speedy deletion. if the image is copyrighted under a non-free license (per wikipedia:fair use) then the image will be deleted 48 hours after . if you have any questions please ask them at the media copyright questions page. thank you.  \"\n",
      "********************\n",
      "bbq   be a man and lets discuss it-maybe over the phone?\n",
      "********************\n",
      "hey... what is it.. @ | talk . what is it... an exclusive group of some wp talibans...who are good at destroying, self-appointed purist who gang up any one who asks them questions abt their anti-social and destructive (non)-contribution at wp?  ask sityush to clean up his behavior than issue me nonsensical warnings...\n",
      "********************\n",
      "before you start throwing accusations and warnings at me, lets review the edit itself-making ad hominem attacks isn't going to strengthen your argument, it will merely make it look like you are abusing your power as an admin.  now, the edit itself is relevant-this is probably the single most talked about event int he news as of late. his absence is notable, since he is the only living ex-president who did not attend. that's certainly more notable than his dedicating an aircracft carrier.  i intend to revert this edit, in hopes of attracting the attention of an admin that is willing to look at the issue itself, and not throw accusations around quite so liberally. perhaps, if you achieve a level of civility where you can do this, we can have a rational discussion on the topic and resolve the matter peacefully.\n",
      "********************\n",
      "oh, and the girl above started her arguments with me. she stuck her nose where it doesn't belong. i believe the argument was between me and yvesnimmo. but like i said, the situation was settled and i apologized. thanks,\n",
      "********************\n",
      "\"  juelz santanas age  in 2002, juelz santana was 18 years old, then came february 18th, which makes juelz turn 19 making songs with the diplomats. the third neff to be signed to cam's label under roc a fella. in 2003, he was 20 years old coming out with his own singles \"\"santana's town\"\" and \"\"down\"\". so yes, he is born in 1983. he really is, how could he be older then lloyd banks? and how could he be 22 when his birthday passed? the homie neff is 23 years old. 1983 - 2006 (juelz death, god forbid if your thinking about that) equals 23. go to your caculator and stop changing his year of birth. my god.\"\n",
      "********************\n",
      "bye!   don't look, come or think of comming back! tosser.\n",
      "********************\n",
      "voydan pop georgiev- chernodrinski\n",
      "********************\n",
      "the mitsurugi point made no sense - why not argue to include hindi on ryo sakazaki's page to include more information?\n",
      "********************\n",
      "don't mean to bother you   i see that you're writing something regarding removing anything posted here and if you do oh well but if not and you can acctually discuss this with me then even better.  i'd like to ask you to take a closer look at the premature wrestling deaths catagory and the men listed in it, surely these men belong together in some catagory. is there anything that you think we can do with the catagory besides delting it?\n",
      "********************\n",
      "\"   regarding your recent edits   once again, please read wp:filmplot before editing any more film articles.  your edits are simply not good, with entirely too many unnecessary details and very bad writing.  please stop before you do further damage. -''''''the '45 \"\n",
      "********************\n",
      "\" good to know. about me, yeah, i'm studying now.(deepu) \"\n",
      "********************\n",
      "\"   snowflakes are not always symmetrical!   under geometry it is stated that \"\"a snowflake always has six symmetric arms.\"\" this assertion is simply not true! according to kenneth libbrecht, \"\"the rather unattractive irregular crystals are by far the most common variety.\"\"   someone really need to take a look at his site and get facts off of it because i still see a decent number of falsities on this page. (forgive me im new at this and dont want to edit anything)\"\n",
      "********************\n",
      "\"   the signpost: 24 september 2012    read this signpost in full  single-page  unsubscribe     \"\n",
      "********************\n",
      "\"  re-considering 1st paragraph edit? i don't understand the reasons for 's recent edit of this article  not that i'm sure that the data are necessarily \"\"wrong.\"\"  rather, i'm persuaded that the strategy of introducing academic honors in the first paragraph is an unhelpful approach to this specific subject.  i note that articles about other sitting justices have been similarly \"\"enhanced;\"\" and i also believe those changes are no improvement.    in support of my view that this edit should be reverted, i would invite anyone to re-visit articles written about the following pairs of jurists.  a1. benjamin cardozo  a2. learned hand   b1. john marshall harlan  b2. john marshall harlan ii  the question becomes: would the current version of the wikipedia article about any one of them  or either pair  be improved by academic credentials in the introductory paragraph?  i think not.  perhaps it helps to repeat a wry argument kathleen sullivan of stanford law makes when she suggests that some on the harvard law faculty wonder how antonin scalia avoided learning what others have managed to grasp about the processes of judging?  i would hope this anecdote gently illustrates the point.   less humorous, but an even stronger argument is the one clarence thomas makes when he mentions wanting to return his law degree to yale.  at a minimum, i'm questioning this edit?  it deserves to be reconsidered.   \"\n",
      "********************\n",
      "radial symmetry   several now extinct lineages included in the echinodermata were bilateral such as homostelea, or even asymmetrical such as cothurnocystis (stylophora).  -\n",
      "********************\n",
      "there's no need to apologize. a wikipedia article is made for reconciling knowledge about a subject from different sources, and you've done history studies and not archaeology studies, i guess. i could scan the page, e-mail it to you, and then you could ask someone to translate the page.\n",
      "********************\n",
      "yes, because the mother of the child in the case against michael jackson was studied in here motives and reasonings and judged upon her character just as harshly as wacko jacko himself.  don't tell me to ignore it and incriminate myself.  i am going to continue refuting the bullshit that jayjg keeps throwing at me.   ,\n",
      "********************\n",
      "\" ok. but it will take a bit of work but i can't quite picture it. do you have an example i can base it on?  the duck \"\n",
      "********************\n",
      "\"== a barnstar for you! ==    the real life barnstar lets us be the stars    \"\n",
      "********************\n",
      "how could i post before the block expires?  the funny thing is, you think i'm being uncivil!\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "\n",
    "for i in range(len(train.comment_text)):\n",
    "    comment = cleaning_text(train.comment_text[i])\n",
    "    print(comment)\n",
    "    print('*' * 20)\n",
    "    cnt += 1\n",
    "        \n",
    "    if cnt > 30:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"== TfD nomination of Template:SilentRedirect ==\n",
      "\n",
      "Template:SilentRedirect has been nominated for deletion. You are invited to comment on the discussion at Wikipedia:Templates for deletion#Template:SilentRedirect. Thank you.   | (talk) \n",
      "\n",
      " Watermarks \n",
      "\n",
      "I shall remove the obvious ones, the less obvious ones stay. I am allowed to watermark images with anything whatsoever I wish to, even if it's a contradiction, and if you wish me to use up further WP  bandwidth by re-uploading them, that has no effect on me. I can't do anything until this evening, and if any get deleted, I shall reupload them. Yours without respect (talk|email) \n",
      "You could have the grace to reply. I said that I'd deal with them this evening, but only the ones where the watermark is visible from the page. So *** off (talk|email) \n",
      "I am planning to re-upload them with less obtrusive watermarks, such as that on Image:Bliss parody.jpg. (talk|email) \n",
      "It wasn't vandalism; I used an external link as per your instructions, which incidentally I'm not bound to follow. I am certainly on the verge of requesting mediation, as you are disrupting my life, never mind WP, to make a point. I will tell you one more time: any images are better than none. If you wish to get your own pix of all the concerned images, you're damn welcome to try, but they are GFDL therefore there's no grounds for removal. (talk|email) \n",
      "PS - I shall add the external link again, not as an image, and if you remove it I shall make a formal complaint to the Wikimedia Foundation, and the Information Commissioner in London. (talk|email) \n",
      "\n",
      ", your conduct is a little off in this matter. While I'm all for discussing the issues with users, let's keep it civil please? Thanks.  (talk) \n",
      "\n",
      " I disagree, Rob - it is far more than \"\"a little\"\" off. Images watermarked as such are not in any way appropriate for Wikipedia. This is an encyclopdia, not a pet image project. If \"\"TheDoctor10\"\" isn't happy to play ball, he can go elsewhere.\n",
      "  (talk) \n",
      "\n",
      "I am proposing a change to the image use policy to forbid watermarked images. Please voice an opinion at Wikipedia_talk:Image_use_policy#User-created_images -Thanks - talk \n",
      "\n",
      "I edit from the United Kingdom, and under the 1990 Computer Misuse Act, the editing of information online is illegal, without permission. The button at the top of every page that says \"\"Edit this page\"\", among other things, constitutes that permission, while the policies and guidelines form conditions. Since there is no condition against my image, it has a legal right to be there. Q.E.D. (talk|email) \n",
      "True, but therefore your addition fails under the same logic. Also, Wikimedia own the servers, it's their right to delete it. 13ID:540053 \n",
      "Apologies to Ed for invading his talk page, but what a load of rubbish! Wikipedia are under no legal obligation whatsoever to host your images, regardless of whether they meet any policy. I have no idea under what flawed logic you presume that the fact you can edit Wikipedia means that your edits must legally be accepted. If the Computer Misuse Act states that the editing of information online is illegal without permission, that implies that editing with permission is legal, and nothing more. Furthermore, by your logic, I have the legal right to edit Wikipedia, and remove your images... I fail to see what on earth you intend to tell your lawyer, but I don't imagine the Wikimedia board are too concerned... ''''''/talk \n",
      "\n",
      "Ignoring the gibberish i\n",
      "********************\n",
      "\"\n",
      "OK, I've sent: Please may Wikipedia (www.wikipedia.org) use a mix of the new, Murray Gold, Middle 8 and the regular theme on their article on Doctor Who theme music (en.wikipedia.org/wiki/Doctor_Who_theme_music)? It is quite long, but \"\"gets the atmosphere across\"\". Wikipedia is a project often linked to from BBC News articles, and occasionally the subject of them (news.bbc.co.uk/1/hi/technology/4534712.stm). Please reply, A Nonymous. (talk|email) \"\n",
      "********************\n",
      "Feb 10 \n",
      "\n",
      " (talk|email)\n",
      "********************\n",
      "OK, I shall do so in the morning, or if you want, you can, since you probably won't beleive me if I say that they said yes. (talk|email)\n",
      "********************\n",
      "Yep, definitely Jacobs.00 (talk|email)\n",
      "********************\n",
      "It would be polite to, yes. (talk|email)\n",
      "********************\n",
      "Shall I ask the Beeb's permission, then? (talk|email)\n",
      "********************\n",
      "I said that it's a mix of the various bits, anyway, I'm sure we'll email back and fourth a few times before I get permission, so I'll tell then if they specifically ask. However, I feel that it's still fair use. (talk|email)\n",
      "********************\n",
      "Ignoring the gibberish immediately above this, I must compliment you on your sense of humour - in fact the reason I didn't reply immediately was that I was at my local hospital's emergency room having my sides stitched back together. (talk|email)\n",
      "********************\n",
      "\"WikiThanks.png]] Hi Dan, thank you for voting in support of my RFA; the result was (28-0-0 ). I hope that I am able to fulfil the expectations of an admin. If you see me mess up anywhere, have any concerns, please don't hesitate to tell me! Take care.  (Talk) \n",
      "\n",
      "PS: Hope Wikinews is going well.\n",
      "\n",
      " RC patrolling response \n",
      "What I've been doing the past few days actually is monitoring the new articles page. I only found out about the RC IRC dump tonight, so yes, that's what I've been doing. -)  \n",
      "Then again, how'd you notice?  \n",
      "\n",
      " Great job \n",
      "\n",
      "  I award this Barnstar to Dan100 for your great job doing just about all patrolling in #wikipedia-en-vandalism for a while. Doing a great job with no one else patrolling most of the time. Doing a lot at time and beating me to reverting vandalism as well during this time as well  Talk [/w/index.php?title=User_talk:Adam1213&action;=edit&section;=new +] \n",
      "\n",
      " {{test}} \n",
      "\n",
      "Hi, this IP (194.154.22.51) is registered to Tiffin Boys' School  and I got the message despite not having vandalised, but I'll \"\"have a word\"\" with the people in the IT rooms who may have done it. Sorry, but pls don't block, as it's used by at least 20ppl. Thanks, JS\n",
      "\n",
      " 209.175.175.11 \n",
      "\n",
      "Could you block them, please? More vandalism to Sarcou...I don't know how to spell it:-) I emailed the abuse email address. (talk|email) \n",
      "No problem. (talk|email) \n",
      "\n",
      " Thanks \n",
      "\n",
      "Thank you watching my back (reverts on my user and talk pages).  \n",
      "\n",
      " Britt Hackemack \n",
      "Hi, I use the IRC RC dump for vandalism monitoring\"\n",
      "********************\n",
      "WP:AIV \n",
      "\n",
      "You'll be blocked now. (talk|email)\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "\n",
    "for i in range(len(train.comment_text)):\n",
    "    if re.findall('\\(talk\\|email\\)', train.comment_text[i]):\n",
    "        print(train.comment_text[i])\n",
    "        print('*' * 20)\n",
    "        cnt += 1\n",
    "        \n",
    "    if cnt > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('06', '', '', ', ')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(pattern, train.comment_text[comment_list[7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"06, 29 December 2007 (UTC)\\nYep. LOL, the [[Reformist Party (Serbia)|Reformist Party] is having another go (the 20th very last on the parliamentary election, winning less votes than notable to actually be mentioned). ) \\nBy the way, here's something very little people have figured out - the new Constitution of Serbia has been brought to enable Kosovo' secession. The 1990 Constitution barred that as a possibility, and after the Kumanovo Military-Technical Agreement was signed between NATO and FRY the SRS broke its coalition with SPS and the government collapsed, causing new elections - because that was unconstitutional, as an act of highest treason, enough to be tried from maximum sentence (which the Radicals demanded from then to his death, to have a trial in Serbia, and to be tried for treason among other reasons). This constitution releases the authorities from that weight, and they won't have to go to prison if they recognize any form of further loss of sovereignty in Kosovo. ;)   15\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.comment_text[comment_list[7]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* URL\n",
    "* REDIRECT Talk:\n",
    "* (UTC)\n",
    "* IP address\n",
    "* Date, Time\n",
    "* File name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.imdb.com/name/nm2551199/filmoseries#tt1327666 71.223.125.139'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.comment_text[6193]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic            1\n",
       "severe_toxic     0\n",
       "obscene          1\n",
       "threat           0\n",
       "insult           0\n",
       "identity_hate    0\n",
       "Name: 1373, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[1373, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17         REDIRECT Talk:Voydan Pop Georgiev- Chernodrinski\n",
       "62                        REDIRECT Talk:Frank Herbert Mason\n",
       "87          Oh, it's me vandalising?xD See here. Greetings,\n",
       "146       Azari or Azerbaijani? \\n\\nAzari-iranian,azerba...\n",
       "177          86.29.244.57|86.29.244.57]] 04:21, 14 May 2007\n",
       "351                 Future Perfect at Sunrise|]] 14:59, 16\n",
       "358               I'm afraid that's a broken link for me. -\n",
       "592                         REDIRECT Talk:Jos Manuel Rojas\n",
       "702       Valerie Poxleitner \\n\\nValeri Poxleitner, A.K....\n",
       "758                                  |listas = Manos Family\n",
       "823       Barnes                  Aus     1             ...\n",
       "829                                06:15, 19 Aug 2004 (UTC)\n",
       "852                                 P.S. Are you a /b/tard?\n",
       "886                 Ion G Nemes|talk]]) 04:08, 21 June 2011\n",
       "897                         \"\\nNo problem at all! (talk) \"\n",
       "899                                     I've just seen that\n",
       "916       \"\\nNo problem. Thanks for letting me know.  (t...\n",
       "917       Batman  \\n\\nI am Batman. You are Spiderman. I ...\n",
       "1015                             WikiDon, STOP stalking me!\n",
       "1083                        Type 3 looks gorgeous )  (talk)\n",
       "1177                              |listas = Schaefer, Nolan\n",
       "1192       \"::I LOL'd hardest at J.delanoy's. P Cobra \\n\\n\"\n",
       "1217                            05 /\\n8:  08:17, 12 August\n",
       "1230                                  final unblock request\n",
       "1242       \"\\nre this, it looks like Ultrabias/DY71. ^_^  \"\n",
       "1259                                Why I'm talking to you?\n",
       "1307           -Anonymous Anti-Vandalism Vigilant Vigilante\n",
       "1325      Uh oh, somebodies got internet muscles.99.235....\n",
       "1373                       KISS MY DICK \\n\\n...ALL 3 OF YOU\n",
       "1394      REDIRECT Talk:195455 Mexican Primera Divisin...\n",
       "                                ...                        \n",
       "158570    STOP POSTING WARNIGN ON MY TALK PAGE ITS FUCKI...\n",
       "158589                                db|my RIGHT to vanish\n",
       "158631                            Screw you.149.254.186.158\n",
       "158670                       No problem!  (talk  contribs)\n",
       "158677                      06:41, September 11, 2005 (UTC)\n",
       "158712                         Emmanuel A. Asiegbu]]  22:41\n",
       "158717    stupid head \\n\\nYOur dumb and you are stupid d...\n",
       "158782                            [by user:Laomei, removed]\n",
       "158884                    \"Sticky Parkin|Parkin]] 17:38, 5\"\n",
       "158965    German Sadchenkov \\n\\nRussian version?))) look...\n",
       "158970    Hi Wikipedia!!Hi Wikipedia!!Hi Wikipedia!!Hi W...\n",
       "158988                  YES. I understand.\\n\\nYES, YES, YES\n",
       "159016    1 Nephi 11:13, 1 Nephi 12:23, 1 Nephi 13:15, 2...\n",
       "159019                   Why don't you do it? 24.68.148.215\n",
       "159032    \"\\n\\n Excellent User Page Award \\n\\n Excellent...\n",
       "159045                                 Shut up, you button!\n",
       "159101                                  Yes, someone can. -\n",
       "159103    OH DEAR OH DEAR, HOW FUKIN SAD. I WILL REGRET ...\n",
       "159187      \"\\n\\n talkback \\n\\nColdplay Exprt Let's talk \"\n",
       "159243                               wtf \\n\\n  your a queer\n",
       "159268    Hi Bading \\nPutang ina mong bakla ka. Fuck you...\n",
       "159315    http://www.nysun.com/article/23698 - public in...\n",
       "159330                    I LOVE KATILYN JEWELL!!!! <333333\n",
       "159445                     you gotta let somebody love you.\n",
       "159467                                    (talk  contribs)\n",
       "159493                           FUCKING FAGGOT \\n\\nLOLWAT.\n",
       "159497    I am not a martyr. I am a human, and humans ma...\n",
       "159515                      REDIRECT Talk:Bangkok Bank F.C.\n",
       "159548                          Talkback: 15 September 2012\n",
       "159549                           2005 (UTC)\\n 06:35, 31 Mar\n",
       "Name: comment_text, Length: 4316, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.comment_text[not_eng]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
