{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langdetect import detect\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import gensim\n",
    "from dictionary import contraction_map, unnecessary_patterns\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "sample = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "train_y = train.as_matrix()[:, 2:].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_object = re.compile(\"|\".join(contraction_map.keys()))\n",
    "sub_patterns = '|'.join(unnecessary_patterns)\n",
    "\n",
    "def expand_contraction(sentence, sub_map=contraction_map, sub_object=contraction_object):\n",
    "    def matching_case(match):\n",
    "        return sub_map[match.group(0)]    \n",
    "    return sub_object.sub(matching_case, sentence)\n",
    "\n",
    "\n",
    "def cleaning_text(text, sub_patterns=sub_patterns):\n",
    "    text = re.sub(sub_patterns, ' ', text)\n",
    "    text = re.sub('[0-9]+', 'NUM', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def tokenizer(sentence):\n",
    "    tokenized_sentence = word_tokenize(sentence)\n",
    "    return [wnl.lemmatize(token) for token in tokenized_sentence if token not in stop_words]\n",
    "\n",
    "\n",
    "def preprocessing(dataset):\n",
    "    comment_list = []\n",
    "    for comment in dataset.comment_text:\n",
    "        comment = comment.lower()\n",
    "        comment_list.append(cleaning_text(expand_contraction(comment)))\n",
    "    return comment_list\n",
    "\n",
    "\n",
    "def persistence(fname, mode='load', obj=None):\n",
    "    if mode == 'load':\n",
    "        with open(fname, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    elif mode == 'save' and obj is not None:\n",
    "        with open(fname, 'wb') as f:\n",
    "            pickle.dump(obj, f)\n",
    "            \n",
    "\n",
    "def make_submission(sample_sub, fname, prediction):\n",
    "    idx = sample['id']\n",
    "    columns = sample.columns.tolist()[1:]\n",
    "    sub = pd.DataFrame(prediction, index=idx, columns=columns)\n",
    "    sub.to_csv('submissions/{}.csv'.format(fname), index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_comment = persistence('train_comment.pkl', 'load')\n",
    "test_comment = persistence('test_comment.pkl', 'load')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_comment = preprocessing(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "glove_file = datapath('/Users/youncheol/Documents/projects/toxic-comment-classification-challenge/embedding/glove.twitter.27B.100d.txt')\n",
    "tmp_file = get_tmpfile('glove_model.txt')\n",
    "\n",
    "glove2word2vec(glove_file, tmp_file)\n",
    "\n",
    "glove_model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_model.vocab['motherfucker'].index\n",
    "# glove_model.index2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = list(map(tokenizer, train_comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(glove_model.vocab)\n",
    "sample_size = len(tokenized_train)\n",
    "median_length = np.median(list(map(len, tokenized_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.zeros((sample_size, median_length), dtype='int32')\n",
    "\n",
    "for i, sentence in enumerate(tokenized_train):\n",
    "    for j, word in enumerate(sentence):\n",
    "        if j <= 17:\n",
    "            try:\n",
    "                train_x[i][j] = glove_model.vocab[word].index\n",
    "            except KeyError:\n",
    "                train_x[i][j] = vocab_size\n",
    "        else:\n",
    "            continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
